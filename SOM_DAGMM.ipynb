{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "from minisom import MiniSom\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from som_dagmm.model import DAGMM, SOM_DAGMM\n",
    "from som_dagmm.compression_network import CompressionNetwork\n",
    "from som_dagmm.estimation_network import EstimationNetwork\n",
    "from som_dagmm.gmm import GMM, Mixture\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from SOM import som_train, som_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição dos parâmetros para treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 1024\n",
    "save_path = 'saves/save/epoch_' # Caminho para salvar o modelo\n",
    "dataset = 'IDS2018'             # Dataset a ser treinado e validado\n",
    "features = 'numerical'\n",
    "embed = 'label_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante o carregamento dos dados, já há o split entre os dados de ataque e normais, a fim de facilitar o trabalho a depender do teste que está sendo realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'IDS2018':\n",
    "    data = load_data('data/NEW-CSE-CIC-IDS2018/new-500k.csv')\n",
    "\n",
    "    dataB = data[(data['Label'] ==  \"Benign\")] #Pegando somente os benignos\n",
    "    dataM = data[(data['Label'] !=  \"Benign\")]\n",
    "    categorical_cols = []\n",
    "    YB = get_labels(dataB, dataset)\n",
    "    YM = get_labels(dataM, dataset)\n",
    "\n",
    "if dataset == 'IDS2019':\n",
    "    data = load_data('data/CSE-CIC-IDS2018/CSE-CIC-IDS2019_23.csv')\n",
    "\n",
    "    # Retirando dados que não auxiliam no treinamento\n",
    "    data = data.drop(['Flow ID', 'Source IP', 'Source Port', 'Destination IP','Destination Port', 'Protocol', 'Timestamp', 'SimillarHTTP'], axis='columns')\n",
    "    dataB = data[(data['Label'] ==  \"BENIGN\")] #Pegando somente os benignos\n",
    "    dataM = data[(data['Label'] !=  \"BENIGN\")]\n",
    "    categorical_cols = []\n",
    "\n",
    "    YB = get_labels(dataB, dataset)\n",
    "    YM = get_labels(dataM, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renomeia algumas variáveis para facilitar o reuso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataM\n",
    "Y = YM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento dos dados de entradak, o primeiro conjunto de if's tem como objetivo escolher entre trabalhar com as features categoricas ou não do dataset. Já o segundo conjunto é realizar a conversão dessas features categoricas para numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select features\n",
    "if features == \"categorical\":\n",
    "    data = data[categorical_cols]\n",
    "    dataB = dataB[categorical_cols]\n",
    "if features == \"numerical\":\n",
    "    data = remove_cols(data, categorical_cols)\n",
    "    dataB = remove_cols(dataB, categorical_cols)\n",
    "\n",
    "#encode categorical variables\n",
    "if embed == 'one_hot':\n",
    "    data = one_hot_encoding(data, categorical_cols)\n",
    "    dataB = one_hot_encoding(dataB, categorical_cols)\n",
    "if embed == 'label_encode':\n",
    "    data = label_encoding(data, categorical_cols)\n",
    "    dataB = label_encoding(dataB, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape[0])\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código abaixo, iremos tratar respectivamente\n",
    "- A existência de valores infinitos, os quais são convertidos para nan\n",
    "- A troca de valopes nan por zero, dentro do fill_na\n",
    "- Por fim a normalização das colunas (Utilizando o código realizado pelo artigo, para isso ele utiliza um MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with NA values\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dataB.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "data = fill_na(data)\n",
    "dataB = fill_na(dataB)\n",
    "\n",
    "# normalize data\n",
    "data = normalize_cols(data)\n",
    "dataB = normalize_cols(dataB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando os Dados em Treino e Validação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo é realizado o split dos dados conforme explicitado no artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and train split\n",
    "train_data, test_data, Y_train, Y_test = split_data(data, Y, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data, dataB], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.concatenate([Y_test, YB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(train_data.index[-1])\n",
    "# Y_test = Y_test[:-1]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(Y_train))\n",
    "print(len(test_data))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversão dos dados para tensor Torch, a fim de que eles possam ser utilizados no treinamento e validação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to torch tensors\n",
    "dataX = torch.tensor(data.values.astype(np.float32))\n",
    "train_dataT = torch.tensor(train_data.values.astype(np.float32))\n",
    "test_dataT = torch.tensor(test_data.values.astype(np.float32))\n",
    "\n",
    "#Convert tensor to TensorDataset class.\n",
    "dataset = TensorDataset(train_dataT)\n",
    "\n",
    "#TrainLoader\n",
    "dataloader = DataLoader(train_dataT, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e Validação do SOM-DAGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme explicitado no artigo, primeiramente há o treinamento do SOM e após esse treinamento sua saída é colocada junto ao DAGMM para realizar as predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_som = som_train(data=train_dataT, x=10, y=10, sigma=1, learning_rate=0.8, iters=10000, neighborhood_function= 'gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo há o treinamento do DAGMM, bem como a delcaração dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression = CompressionNetwork(dataX.shape[1])\n",
    "estimation = EstimationNetwork()\n",
    "gmm = GMM(2,6)\n",
    "mix = Mixture(6)\n",
    "dagmm = DAGMM(compression, estimation, gmm)\n",
    "net = SOM_DAGMM(dagmm, pretrained_som)\n",
    "optimizer =  optim.Adam(net.parameters(), lr=0.0001)\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        out = net(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Calculo do loss\n",
    "        L_loss = compression.reconstruction_loss(data[0])\n",
    "        G_loss = mix.gmm_loss(out=out, L1=0.1, L2=0.005)\n",
    "\n",
    "        loss = L_loss + G_loss\n",
    "        \n",
    "\n",
    "        #Retropropagação da loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isfinite(loss):\n",
    "            running_loss += loss.item()\n",
    "            print(f\"LOSS: {loss.item()} - L: {L_loss} - G: {G_loss}\")\n",
    "        else:\n",
    "            print(f\"ERROR: {loss.item()} - L: {L_loss} - G: {G_loss}\")\n",
    "\n",
    "\n",
    "    if (epoch+1) % 5 == 0: #Salva o modelo a cada 5 épocas\n",
    "        path = save_path + str(epoch+1)\n",
    "        torch.save(net, path)\n",
    "\n",
    "        # Avalia o resultado do treinamento provisório\n",
    "        net.eval()\n",
    "        out_ = net(test_dataT)\n",
    "        threshold = np.percentile(out_, 20)\n",
    "        pred = (out_ > threshold).numpy().astype(int)\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        a, p, r, f, auc = get_scores(pred, Y_test)\n",
    "        print(\"Accuracy:\", a, \"Precision:\", p, \"Recall:\", r, \"F1 Score:\", f, \"AUROC:\", auc)\n",
    "            \n",
    "        print(running_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimo eval para verificar o desempenho final do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "out_ = net(test_dataT)\n",
    "threshold = np.percentile(out_, 20)\n",
    "pred = (out_ > threshold).numpy().astype(int)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "a, p, r, f, auc = get_scores(pred, Y_test)\n",
    "print(\"Accuracy:\", a, \"Precision:\", p, \"Recall:\", r, \"F1 Score:\", f, \"AUROC:\", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
